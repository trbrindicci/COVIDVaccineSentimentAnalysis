---
title: "R Notebook"
output: html_notebook
---

```{r}
#install.packages("ROAuth")
#install.packages("hms")
#install.packages("lubridate")
#install.packages("tidytext")
#install.packages("tm")
#install.packages("wordcloud")
#install.packages("igraph")
#install.packages("glue")
#install.packages("networkD3")
#install.packages("plyr")
#install.packages("stringr")
#install.packages("ggeasy")
#install.packages("magrittr")
#install.packages("jeneaustenr")
#install.packages("widyr")
#install.packages("twitterR")
#install.packages("RColorBrewer")
#install.packages("wordcloud2")
```

```{r}
library(ROAuth)
library(lubridate) 
library(tidytext)
library(tm)
library(wordcloud)
library(igraph)
library(networkD3)
library(rtweet)
library(plyr)
library(stringr)
library(ggplot2)
library(ggeasy)
library(plotly)
library(dplyr)  
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)
library(quanteda)
library(writexl)
library(tidytext)
library(caTools)
require(devtools)
library(wordcloud2)
```

```{r}
#Building database with Twitter API. Retry on limit ON since it allows 18k tweets every 15 min, at the end of 15 minutes, code will automatically retry

#vaccineSample <- search_tweets("vaccine", n = 100000, include_rts = FALSE, retryonratelimit = TRUE)

#write_xlsx(vaccine, "/Users/tobiasrodriguezbrindicci/Desktop/Final Project FL2/resources/vaccine")
#write.csv(vaccine, "/Users/tobiasrodriguezbrindicci/Desktop/Final Project FL2/resources/vaccinecsv", row.names = FALSE)

vaccine <- read_excel("vaccine")

#Make a copy just in case
vaccine3 <- vaccine
#Make a smaller sample
vaccine2 <- vaccine[1:10000,]
```

```{r}

#Create a function to clean all tweets
clean.text = function(x)
{
  # remove rt
  x = gsub("rt", "", x)
  # remove numbers
  x = gsub("[[:digit:]]", "", x)
  # remove links http
  x = gsub("http\\w+", "", x)
  x = gsub('https://','',x)
  x = gsub('http://','',x)
  # remove tabs
  x = gsub("[ |\t]{2,}", "", x)
  # remove blank spaces at the beginning
  x = gsub("^ ", "", x)
  # remove blank spaces at the end
  x = gsub(" $", "", x)
  return(x)
}

cleanText <- clean.text(vaccine$text)
cleanText3 <- clean.text(vaccine2)
```


#Basic graphs for introduction
```{r}
#Clean date and time format
vaccine %<>% 
  mutate(
    created = created_at %>% 
      # Remove zeros.
      str_remove_all(pattern = '\\+0000') %>%
      # Parse date.
      parse_date_time(orders = '%y-%m-%d %H%M%S')
  )

vaccine %<>% 
  mutate(CreatedAt = created_at %>% round(units = 'hours') %>% as.POSIXct())

#Get min and max date from dataset
vaccine %>% pull(created_at) %>% min()
vaccine %>% pull(created_at) %>% max()
```

```{r}
plt <- vaccine %>%
  dplyr::count(CreatedAt) %>% 
  ggplot(mapping = aes(x = CreatedAt, y = n)) +
    geom_line(color="#69b3a2") + 
  ggtitle('Number of Tweets per Hour') +
  xlab(label = 'Date') +
  ylab(label = '# of Tweets') +
    theme(
      legend.position="none",
    )

plt %>% ggplotly()
```

```{r}
vaccine2 <- vaccine2 %>% select("text") # selects just the text column

vaccine2$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "",vaccine2$text) # remove alphanumeric characters 
vaccine2$text <- gsub("https\\S*", "",vaccine2$text) # remove hyperlinks
vaccine2$text <- gsub("amp", "",vaccine2$text) # amp just keeps showing up, remove it!!
```

```{r}
#create a corpus to allow us clean the text column with tm
tweets.corpus <- Corpus(VectorSource(vaccine2$text))

tweets.corpus <- tweets.corpus %>%
  tm_map(removeNumbers) %>% # removes numbers from text
  tm_map(removePunctuation) %>% # removes punctuation from text
  tm_map(stripWhitespace) %>% # trims the text of whitespace
  tm_map(content_transformer(tolower)) %>% # convert text to lowercase
  tm_map(removeWords,stopwords("english")) %>% # remove stopwords
  tm_map(removeWords,stopwords("SMART")) # remove stopwords not removed from previous line
```

```{r}
tdm <- TermDocumentMatrix(tweets.corpus) %>% # create a term document matrix
  as.matrix()

words <- sort(rowSums(tdm), decreasing = TRUE) # count all occurences of each word and group them
df <- data.frame(word = names(words), freq = words) # convert it to a dataframe
head(df) # visulaise!
```
```{r}
set.seed(1234) # for reproducibility, sorta
wcloud <- wordcloud2(df,   # generate word cloud
                     size = 3,
                     color= 'random-dark', # set colors
                     shape = 'circle',
                     rotateRatio = 0) #horizontal looks better, but what do you think?
wcloud
```









```{r}
positive = scan('/Users/tobiasrodriguezbrindicci/Desktop/Final Project FL2/resources/positive-words.txt', what = 'character', comment.char = ';')
negative = scan('/Users/tobiasrodriguezbrindicci/Desktop/Final Project FL2/resources/negative-words.txt', what = 'character', comment.char = ';')
# add your list of words below as you wish if missing in above read lists
pos.words = c(positive,'vaccinated','antibody','healthy', 'vaxxed', 'vaxed')
neg.words = c(negative,'wtf','fucking','unvaccinated','anti','antivax','unvaxed',
              'arrest','no','not')
```

```{r}
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
  require(plyr)
  require(stringr)
  
  # we are giving vector of sentences as input. 
  # plyr will handle a list or a vector as an "l" for us
  # we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    
    # clean up sentences with R's regex-driven global substitute, gsub() function:
    sentence = gsub('https://','',sentence)
    sentence = gsub('http://','',sentence)
    sentence = gsub('[^[:graph:]]', ' ',sentence)
    sentence = gsub('[[:punct:]]', '', sentence)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub('\\d+', '', sentence)
    sentence = str_replace_all(sentence,"[^[:graph:]]", " ")
    # and convert to lower case:
    sentence = tolower(sentence)
    
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    
    # compare our words to the dictionaries of positive & negative terms
    pos.matches = match(words, pos.words)
    neg.matches = match(words, neg.words)
    
    # match() returns the position of the matched term or NA
    # we just want a TRUE/FALSE:
    pos.matches = !is.na(pos.matches)
    neg.matches = !is.na(neg.matches)
    
    # TRUE/FALSE will be treated as 1/0 by sum():
    score = sum(pos.matches) - sum(neg.matches)
    
    return(score)
  }, pos.words, neg.words, .progress=.progress )
  
  scores.df = data.frame(score=scores, text=sentences)
  return(scores.df)
}
```

```{r}
analysis <- score.sentiment(cleanText, pos.words, neg.words)
# sentiment score frequency table
table(analysis$score)
```

```{r}
analysis %>%
  ggplot(aes(x=score)) + 
  geom_histogram(binwidth = 1, fill = "lightblue")+ 
  xlim(-5,5) +
  ylab("Frequency") + 
  xlab("sentiment score") +
  ggtitle("Distribution of Sentiment scores of the tweets") +
  ggeasy::easy_center_title()
```

```{r}
neutral <- length(which(analysis$score == 0))
positive <- length(which(analysis$score > 0))
negative <- length(which(analysis$score < 0))
Sentiment <- c("Positive","Neutral","Negative")
Count <- c(positive,neutral,negative)
output <- data.frame(Sentiment,Count)
output$Sentiment<-factor(output$Sentiment,levels=Sentiment)
ggplot(output, aes(x=Sentiment,y=Count))+
  geom_bar(stat = "identity", aes(fill = Sentiment))+
  ggtitle("Barplot of Sentiment Type")
```
```{r}
#Amount of tweets mentioned status of vaccine
vaccinated <- sum(str_detect(vaccine$text, "[V|v]accinated"))
unvaccinated <- sum(str_detect(vaccine$text, "[U|u]nvaccinated"))
vaccinated
unvaccinated
```

```{r}
#Create df with results
vaccines <- c('Vaccinated', 'Unvaccinated')
counts <- c(vaccinated, unvaccinated)

status <- data.frame(vaccines, counts)
head(status)
```

```{r}
#Status plot
ggplot(status, aes(x=vaccines, y=counts, fill=vaccines))+
geom_bar(stat="identity", position=position_dodge())+
  scale_fill_brewer(palette="Paired")+
  theme_minimal()+
  labs(title = "Status of Vaccination",
       subtitle = "Source: Twitter API",
       x= "Status",
       y= "Count")

```

```{r}
#POLITICS
biden <- sum(str_detect(vaccine$text, "[B|b]iden"))
trump <- sum(str_detect(vaccine$text, "[T|t]rump"))
biden
trump
```

```{r}
#Create df with results
politics <- c('Biden', 'Trump')
countsPolitics <- c(biden, trump)

statusPolitics <- data.frame(politics, countsPolitics)
head(statusPolitics)
```


```{r}
ggplot(status, aes(x=politics, y=countsPolitics, fill=politics))+
geom_bar(stat="identity", position=position_dodge())+
  scale_fill_brewer(palette="Paired")+
  theme_minimal()+
  labs(title = "Tweets Mentioning President Biden and Former President Trump",
       subtitle = "Source: Twitter API",
       x= "Presidents",
       y= "Count")
```







```{r}
library(plyr)
library(scales)
library(purrr)
library(textdata)
library(reshape2)
library(igraph)
library(ggraph)
library(widyr)
library(grid)
library(topicmodels)
```

```{r}
names(vaccine3)[names(vaccine3) == 'text'] <- 'tweet'
dfList<-list(vaccine3)


result_list <- llply(dfList, function(x) {
                #only keep tweets using English as the main language
                x<-subset(x,x$`lang` == "en")
                #change the variable name for future convenience
                names(x)[names(x) == 'text'] <- 'tweet'
                #drop all other variables except tweet
                x<-x[,5]
                #create a new variable to track the number of tweet
                x$tweetnumber <- 1:nrow(x)
                #x$tweetnumber<-1:length(x$tweet) 
                #return the cleaner dataframe with 2 variables
                return(x) 
                })

#apply the function to each dataset
twts1<-as.data.frame(result_list)
```

```{r}
#stop_words is a combination of English stop words from three lexicons, as a data frame. 
data(stop_words)

#customize stop words
custom_stop_words <- bind_rows(
  tibble(word = c("'vaccine'"), 
         lexicon = c("custom")), stop_words)
```

```{r}
#store those special symbols in the variable so we can remove them later
remove_reg <- "&amp;|&lt;|&gt;"

#create a list
dfList2<-list(vaccine3)
result_list2 <- 
  llply(dfList2, function(x) {
    y <- x %>% 
    #remove special symbols for the values under the tweet variable
    mutate(tweet = str_remove_all(tweet, remove_reg)) %>%
    #extract every word from every tweet 
    unnest_tokens(word, tweet, token = "tweets") %>%
    #filter out all stop words
    filter(!word %in% custom_stop_words$word,
           !word %in%str_remove_all(custom_stop_words$word, "'"),
    str_detect(word, "[a-z]"))
    return(y)})

tidy1<-as.data.frame(result_list2)
```

```{r}
#Count the Frequency for Each Word
tidy_week11 <- tidy1 %>%dplyr::count(word, sort = TRUE) 

#Remove all non-english tokens
tidy1_english <- tidy_week11[which(!grepl("[^\x01-\x7F]+", tidy_week11$word)),]
```

```{r}
#create a list 
dfList3<-list(tidy1_english)

#visualize using bar plot
result_list3 <- 
  llply(dfList3, function(x) {
    plot <- x %>%
    #keep only the top 20 tokens
    dplyr::top_n(20) %>%
    #reorder word based on the count
    dplyr::mutate(word = reorder(word, n)) %>%
    #plot using ggplot2
    ggplot(aes(word, n, fill=word)) +
    #specify it's a bar plot
    geom_bar(stat="identity")+
    scale_fill_hue(c=45, l=80)+
    xlab(NULL) +
    coord_flip()+
    theme(legend.position="none")
    return(plot)})

result_list3[[1]]
```
```{r}
#create a list
dfList4<-list(tidy1)

#visualize using Word clouds
result_list_wordclouds <- 
  llply(dfList4, function(x) {
    plot <- x %>%
    inner_join(get_sentiments("bing")) %>%
    count(word, sentiment, sort = TRUE) %>%
    acast(word ~ sentiment, value.var = "n", fill = 0) %>%
    comparison.cloud(colors = c("gray20", "gray80"),max.words = 50)
    return(plot)})
```
```{r}
#Get specific sentiment lexicons in a tidy format, with one row per word, in a form that can be joined with a one-word-per-row data set
get_sentiments("afinn")
```
```{r}
get_sentiments("bing") 
```
```{r}
get_sentiments("loughran")
```

```{r}
#Find the most common positive words using nrc lexicon
loughran_positive <- get_sentiments("loughran") %>% 
  filter(sentiment == "positive")

result_list4 <- 
  llply(dfList4, function(x) {
    plot <- x %>%
    inner_join(loughran_positive) %>%
    dplyr::count(word, sort = TRUE) %>%
    dplyr::top_n(20) %>%
    dplyr::mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n, fill=word)) +
    geom_bar(stat="identity")+
    scale_fill_hue(c=45, l=80)+
    xlab(NULL) +
    coord_flip()+
    theme(legend.position="none")
    return(plot)})

result_list4[[1]]
```
```{r}
#Find the most common positive words using nrc lexicon
loughran_negative <- get_sentiments("loughran") %>% 
  filter(sentiment == "negative")

result_list4 <- 
  llply(dfList4, function(x) {
    plot <- x %>%
    inner_join(loughran_negative) %>%
    dplyr::count(word, sort = TRUE) %>%
    dplyr::top_n(20) %>%
    dplyr::mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n, fill=word)) +
    geom_bar(stat="identity")+
    scale_fill_hue(c=45, l=80)+
    xlab(NULL) +
    coord_flip()+
    theme(legend.position="none")
    return(plot)})

result_list4[[1]]
```
```{r}
#Create a visualization about how much each word contributed to each sentiment.
result_list_contribute <- 
  llply(dfList4, function(x) {
    plot <- x %>%
    inner_join(get_sentiments("bing")) %>%
    dplyr::count(word, sentiment, sort = TRUE)  %>%
    group_by(sentiment) %>%
    top_n(30) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n, fill = sentiment)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~sentiment, scales = "free") +
    labs(y = "Contribution to sentiment",
         x = NULL) +
    coord_flip()
    return(plot)})

result_list_contribute[[1]]
```
```{r}
result_list7 <- 
  llply(dfList4, function(x) {
    plot <- x %>%
    count(word) %>%
    inner_join(get_sentiments("loughran"), by = "word") %>%
    group_by(sentiment) %>%
    top_n(10, n) %>%
    ungroup() %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n, fill=word)) +
    geom_bar(stat="identity")+
    scale_fill_hue(c=45, l=80)+
    coord_flip() +
    facet_wrap(~ sentiment, scales = "free") +
    theme(legend.position="none")
    return(plot)})

result_list7[[1]]
```


```{r}
#Regressions
namesCol <- c("Text","Char Count", "Source", "Retweet Count", "Fav Count")
rowVal <- c(vaccine$text, vaccine$display_text_width, vaccine$source, vaccine$retweet_count, vaccine$favorite_count)

TextCol <- c(vaccine$text)
CharCountCol <- c(vaccine$display_text_width)
SourceCol <- c(vaccine$source)
RtCountCol <- c(vaccine$retweet_count)
FavCountCol <- c(vaccine$favorite_count)

madeSet <- data.frame(TextCol, CharCountCol, SourceCol, RtCountCol, FavCountCol)
madeSet
```

```{r}
set.seed(321)

#split the data
sample <- sample.split(madeSet$SourceCol, SplitRatio = 0.6)
train <- subset(madeSet, sample == TRUE)
test <- subset(madeSet, sample == FALSE)
```

```{r}
ctreemodel <- ctree(CharCountCol ~ RtCountCol + FavCountCol, data = train)

plot(ctreemodel)
```
```{r}
lgModel <- glm(RtCountCol ~ CharCountCol + SourceCol, data = train, family = binomial)
lgModel

new.movies <- data.frame(Budget = 10, NumberOfTheaters = 1000)

pred <- predict(lgModel, new.movies, type = "response")
pred
```



